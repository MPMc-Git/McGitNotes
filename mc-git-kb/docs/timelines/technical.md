---
tags:
  - Technical
  - Timeline
---

<small>Compiled in conjunction with Gemini</small>

!!! danger "**~200 – 60 BCE** [Year Zero]"
    - *Antikythera Mechanism* - Found in a shipwreck in 1901 off the coast of Greece, it is a shoebox-sized bronze device containing over 30 interlocking gears.

!!! info "**1830s – 1940s** [The Foundations]"
    - *1837*: Charles Babbage designs the Analytical Engine. Although never fully built, it was the first design for a general-purpose computer with a "mill" (CPU) and "store" (memory).
    - *1843*: Ada Lovelace writes the first algorithm for the Analytical Engine, becoming the world's first computer programmer.
    - *1936*: Alan Turing publishes "On Computable Numbers," introducing the Turing Machine—the theoretical foundation for all modern digital computers.
    - *1945*: ENIAC is unveiled. It was the first electronic, general-purpose digital computer, weighing 30 tons and using 18,000 vacuum tubes.

!!! info "**1947 – 1960s**
    - *1947*: The Transistor is invented at Bell Labs. This eventually replaced bulky vacuum tubes, allowing computers to become smaller, faster, and more reliable.
    - *1953*: Grace Hopper develops the first compiler, leading to high-level languages like COBOL.
    - *1958*: The Integrated Circuit (Microchip) is co-invented by Jack Kilby and Robert Noyce, enabling the placement of multiple electronic components onto a single sliver of silicon.
    - *1964*: IBM System/360 is launched. This was the first "family" of computers that allowed software to be shared across different models, revolutionizing business computing.

!!! info "**1970s – 1980s**
    - *1971*: Intel 4004 is released—the first commercial Microprocessor. The entire CPU was now on a single chip.
    - *1973*: Xerox Alto is developed. It was the first computer to use a Graphical User Interface (GUI) and a mouse, though it was never sold commercially.
    - *1977*: The "Trinity" of home computing—the Apple II, Commodore PET, and TRS-80—brings computing into the average home.
    - *1984*: The Apple Macintosh popularizes the GUI, making computers intuitive for non-technical users.

!!! info "**1990s – 2010s**
    - *1991*: Tim Berners-Lee releases the code for the World Wide Web, creating a user-friendly layer on top of the existing Internet.
    - *1997*: IBM Deep Blue defeats Garry Kasparov at chess, marking a massive milestone in Artificial Intelligence.
    - *2007*: The iPhone is released, shifting the computing paradigm from the desktop to "Mobile-First" and making the internet ubiquitous via smartphones.
    - *2010s*: The rise of Cloud Computing (AWS, Azure) allows massive computing power to be accessed on-demand over the web rather than on local hardware.

!!! info "**2020 – 2025**
    - *2022–2023*: Large Language Models (LLMs) like GPT-4 and Gemini mainstream Generative AI, changing how humans interact with machines through natural language.
    - *2024–2025*: Agentic AI and specialized NPU (Neural Processing Unit) hardware become standard, allowing computers to perform complex, multi-step tasks autonomously.
    - *Current Frontier*: Quantum Computing reaches "Quantum Utility," where quantum processors begin solving specific scientific problems that are impossible for classical supercomputers.